{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL  Reinforcement learning lectures for everyone "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "(Agent) action, state / reward (Environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q(s,a) = \\mathbb{E} [r + \\gamma \\max_{a'} Q(s',a') \\mid s,a]$\n",
    "(state, action, reward)\n",
    "\n",
    "<br>\n",
    "Q (state, action) <br>\n",
    "Q (s1, LEFT): 0 <br>\n",
    "Q (s1, RIGHT): 0.5 <br>\n",
    "Q (s1, UP): 0 <br>\n",
    "Q (s1, DOWN): 0.3 <br>\n",
    "\n",
    "그럼 여기서 문제 우리는 Q를 어떻게 배우지? <br>\n",
    "\n",
    "Asuume (belive) Q in s' exists! <br>\n",
    "\n",
    "- I am in s <br>\n",
    "- when I do action a, I’ll go to s` <br>\n",
    "- when I do action a, I’ll get reward r <br>\n",
    "- Q in s' , Q(s`, a`) exist! <br>\n",
    "• How can we express Q(s, a) using Q(s`, a`)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-value 업데이트 공식:\n",
    "\n",
    "$\n",
    "Q(s, a) \\leftarrow r + \\max_{a'} Q(s', a')\n",
    "$\n",
    "\n",
    "예시:\n",
    "\n",
    "현재 상태 $ s $에서 행동 $ a $를 선택했을 때, 즉시 보상 $ r = 5 $를 받았습니다.\n",
    "\n",
    "\n",
    "새로운 상태 $ s' $에서 가능한 행동 중 가장 높은 Q-value를 가지는 행동을 선택하여 \n",
    "\n",
    "\n",
    "$\\max_{a'} Q(s', a') = 10$ 이라고 가정합니다.\n",
    "\n",
    "\n",
    "그러면 Q-value 업데이트는 다음과 같이 계산됩니다:\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "$\n",
    "Q(s, a) = 5 + 10 = 15\n",
    "$\n",
    "<br>\n",
    "따라서, 상태 $ s $에서 행동 $ a $를 선택했을 때의 Q-value는 이제 15가 됩니다.\n",
    "\n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow (1-\\alpha) \\cdot Q(s,a) + \\alpha \\cdot \\left( r + \\gamma \\cdot \\max_{a'} Q(s',a') \\right)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    " \n",
    " \n",
    "$$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploit Explore # 슈도 코드 모음\n",
    "\n",
    "#for i in range(1000):\n",
    "#    e = 0.1 / (i + 1)  # 탐험률 계산: 점점 낮아지는 ε-greedy 방식\n",
    "#    if random.random() < e:\n",
    "#        a = random_action()  # 무작위 행동 선택\n",
    "#    else:\n",
    "#        a = argmax(Q(s, A))  # Q 값이 가장 큰 행동 선택 (활용)\n",
    "\n",
    "# random noise\n",
    "\n",
    "#a = argmax(Qs, a_ + random_values)\n",
    "#a = argmax([0.5, 0.6, 0.3, 0.2, 0.5] + [0.1, 0.2, 0.7, 0.3, 0.1])\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-table <br>\n",
    "좋은 방식이지만 계산량이 너무 많다. <br>\n",
    "# Q-Network\n",
    " Q-network 트레이닝은 강화 학습에서 Q-learning 알고리즘을 사용하여 최적의 행동을 학습하는 과정을 신경망을 통해 수행하는 것\n",
    " \n",
    " $ L(\\theta) = \\mathbb{E}_{(s, a, r, s') \\sim \\mathcal{D}} \\left[ \\left( Q(s, a; \\theta) - \\left( r + \\gamma \\max_{a'} Q(s', a'; \\theta^-) \\right) \\right)^2 \\right] $\n",
    "# DQN\n",
    "correlation between sample (experience replay\n",
    "non stationary target\n",
    "\n",
    "\n",
    "1.Go deep \n",
    "\n",
    "\n",
    "2.Capture and replay\n",
    "Correlations between samples\n",
    "\n",
    "\n",
    "3.separate networks: create a target network\n",
    "non-stationary targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강화학습  구현 코드<br>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드는 그냥 삭제하기로 했다.\n",
    "뭐 대충은 \"이거 우리가 생각하는 2차원 평면이랑은 조금 다르게 구성한거 같은데??)(NxNxN 행렬을 구성해서 앞에 2개를 왜 평면으로 쓰는거야!)\n",
    "뭐 여기까지는 사소한 문제였고 내가 조금만 손 보면 보통 사람이 생각하는 형태의 2차원 평면을 구성하는게 가능했는데,\n",
    "그다음부터 action 은 도저히 모르겠다. (이게 내가 생각하기에는 상태별 기록인 것 같은데 여기서 01234 넣으면 상하좌우 움직임이 된다고??)\n",
    "\n",
    "\n",
    "뭐 전체적인 부분은 맞는 것 같더라 탐험율이나 이런것 등등\n",
    "근데 디테일이 너무 틀린게 맞는지 아닌지도 애매해서 포기  \n",
    "내가 직접 구현해보려고 했는데 계속 오류가 나고 이쪽에 시간 너무 많이 쏟아붓는건 아닌 것 같아서 포기\n",
    "Gym인가 그 라이버러리 안 되는게 큰 타격인듯...\n",
    "대부분 그거로만 구현하고 직접 격자점으로 구현하면 오류가 나는 것 같고.\n",
    "\n",
    "\n",
    "교수님 강의는 정말 명강의였다. \n",
    "짧고 핵심만 간결하게 전달 그야말로 \"예고편\" 같은 강의\n",
    "그래서 아쉽다 본편을 보고 싶은데 너무 짧았다.\n",
    "\n",
    "\n",
    "하지만 딱 이정도가 입문자한테 어울리는 수준 애초에 강의도 실험을 제외하면 분량이 2~3시간정도밖에 안 된다.\n",
    "\n",
    "이거 끝났으니까 데이터사이언스 프로그래밍 강의 공부할 차례군..\n",
    "사실 정확하게는 복습이다. 어제 대충 들어보기는 했는데,\n",
    "\n",
    "어제 오랜만에 강의만 들으니까 내가 여태까지 얼마나 엉터리 공부 했는지 느껴지더라...\n",
    "다른건 몰라도 진짜 공부하면서 이렇게 기록 남기면서 하는 건 좋은 습관인듯,\n",
    "예전에는 그냥 무식하게 반복만 했는데"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
